{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Design of the trail:\n",
    "1.Structure\n",
    "     - dimension mapping in the whole space transform process: (#1, #2, [...]). each # corresponds to an independent space, whose dimension is the value for the #.\n",
    "     - design of non-linear scalar-valued function design of each dimension above: polynomial functions.\n",
    "2.Fitting Algorithm\n",
    "     - gradient descent\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "We start by designing the polynomial function of degree 3. To consider all possible terms in polynomials, we can leverage the method of Cartesian product and make some changes on it.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "1. generate final result directly by something like `np.identity`?\n",
    "2. ideas about store type: \n",
    "    - pure ndarray implementation (order is clear).\n",
    "    - set(term_collection) -> set -> list.\n",
    "3. structure design of `term`:\n",
    "    - value('x_1'), power | matching by index -> not clear even possible.\n",
    "    - 'x_1', power | matching by str -> more clear, and matching by str is easy.\n",
    "    - [power1, power2, ...] for each idx -> less clear meaning, more memory, but more clear in structure. \n",
    "\"\"\"\n",
    "def get_terms_collection(quantity_of_variables, max_degree):\n",
    "    if max_degree == 1:\n",
    "        return [np.identity(quantity_of_variables, dtype=np.int32)]\n",
    "\n",
    "    \"\"\"\n",
    "    1. another idea for implementing `terms_collection_s`: add a new arg 'all_pre_terms_collection' in function.\n",
    "    2. `tcfcdl`: terms_collection_for_certain_degree_list\n",
    "    \"\"\"\n",
    "    tcfcdl = get_terms_collection(quantity_of_variables, max_degree - 1)\n",
    "\n",
    "    terms_collection_for_one_smaller_degree = tcfcdl[-1]\n",
    "    terms_collection_for_cur_max_degree = []\n",
    "\n",
    "    for idx in range(quantity_of_variables):\n",
    "        for pre_term in terms_collection_for_one_smaller_degree:\n",
    "            pre_term_ = pre_term.copy()\n",
    "            pre_term_[idx] += 1\n",
    "            terms_collection_for_cur_max_degree.append(pre_term_)\n",
    "        \"\"\"\n",
    "        another idea: generate `term_collector` by filtering something like [1, 2], [2, 1] by set? seems wrong.\n",
    "        \"\"\"\n",
    "        terms_collection_for_one_smaller_degree = list(\n",
    "            filter(lambda term: term[idx] == 0, terms_collection_for_one_smaller_degree)\n",
    "        )\n",
    "    tcfcdl.append(np.array(terms_collection_for_cur_max_degree))\n",
    "    return tcfcdl\n",
    "\n",
    "a = get_terms_collection(2, 2)\n",
    "print(np.vstack(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ref: see `simple_nn.ipynb` \"\"\"\n",
    "config = lambda: 0\n",
    "\n",
    "config.feature_dimension = 2\n",
    "config.space_mapping_process = (config.feature_dimension, 2)\n",
    "config.max_degree = 1\n",
    "config.learning_rate = 1\n",
    "config.batch_size = 100\n",
    "config.steps = 1000\n",
    "\n",
    "class ENN():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "          \n",
    "        \"\"\" `fp_trasformation`: feature_polynomial_transformation \"\"\"\n",
    "        self.fp_transformation  = np.vstack(get_terms_collection(\n",
    "            self.config.feature_dimension, self.config.max_degree))\n",
    "        \n",
    "        \"\"\"\n",
    "        `W`: coefficients_for_all_dimension_in_next_space\n",
    "        len of `fp_transformation`: quantity_of_variable_term\n",
    "        \"\"\"\n",
    "        self.W = np.random.normal(size=(\n",
    "            len(self.fp_transformation), self.config.space_mapping_process[1]))\n",
    "        \n",
    "        \"\"\" `b`: constant_coefficient_for_all_dimension_in_next_space \"\"\"\n",
    "        self.b = np.random.random((self.config.space_mapping_process[1], ))\n",
    "        \n",
    "    def feature_transform(self, feature_input_s):\n",
    "        feature_output_s = []\n",
    "        for x in feature_input_s:\n",
    "            feature_output_s.append([\n",
    "                sum(var ** power if power != 0 else 0 for var, power in zip(x, term)) \n",
    "                    for term in self.fp_transformation\n",
    "            ])            \n",
    "        return np.array(feature_output_s)\n",
    "            \n",
    "    \"\"\" backpropagation \"\"\"\n",
    "    \"\"\" note that `x_s_train` is just an alias of `feature_input_s`. \"\"\"\n",
    "    def fit(self, x_s_train, y_s_train, params_trace_recording=False):\n",
    "        x_s_train, y_s_train = np.array(x_s_train), np.array(y_s_train)\n",
    "        W_trace, b_trace = [self.W.copy()], [self.b.copy()]\n",
    "        sample_size = len(x_s_train)\n",
    "        \n",
    "        for step in range(self.config.steps):\n",
    "            indices_batch = random.sample(range(sample_size), self.config.batch_size)\n",
    "\n",
    "            \"\"\" i corresponds to row in `self.W` and j to col. \"\"\"\n",
    "            \"\"\" define them as function without actual object reference can free memory at soon. and I think it's faster than del statement. \"\"\"\n",
    "            x_s = self.feature_transform(x_s_train[indices_batch])\n",
    "            y_minus_f_s = x_s.dot(self.W) + self.b - y_s_train[indices_batch]\n",
    "\n",
    "            \"\"\" \n",
    "            1. `pd`: partial derivative \n",
    "            2. `l_on_f`: all the loss_j on the f_j\n",
    "            3. `fj_on_w__j`: all the fj on the w_ij when j is fixed\n",
    "            \"\"\"\n",
    "            pd_of_l_on_f_s = 2 / self.config.space_mapping_process[1] * y_minus_f_s\n",
    "            pd_of_fj_on_w__j_s = x_s\n",
    "                        \n",
    "            self.W -= np.mean(self.config.learning_rate * pd_of_fj_on_w__j_s[:, :, np.newaxis] @ pd_of_l_on_f_s[:, np.newaxis, :], axis=0)\n",
    "            self.b -= np.mean(self.config.learning_rate * pd_of_l_on_f_s, axis=0)\n",
    "            \n",
    "            if params_trace_recording:\n",
    "                \"\"\" attention this! \"\"\"\n",
    "                W_trace.append(self.W.copy())\n",
    "                b_trace.append(self.b.copy())\n",
    "        print('training completed.') \n",
    "        return W_trace, b_trace\n",
    "\n",
    "    # forward computing\n",
    "    def predict(self, x_s, custom_params=None):\n",
    "        if custom_params:\n",
    "            return np.argmax(self.feature_transform(x_s).dot(custom_params['W']) + custom_params['b'], axis=1)\n",
    "        else:    \n",
    "            return np.argmax(self.feature_transform(x_s).dot(self.W) + self.b, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training completed.\n"
     ]
    }
   ],
   "source": [
    "# data preparation.\n",
    "sampling_size = 50\n",
    "x = np.linspace(-1, 1, sampling_size)\n",
    "y_1 = 0.5 * np.sin(3.1*(x-0.5)) + 0.4\n",
    "y_2 = 0.5 * np.sin(3.1*(x-0.5)) - 0.45\n",
    "\n",
    "x_s_train = np.array(\n",
    "    list(zip(x, y_1)) +\n",
    "    list(zip(x, y_2))\n",
    ")\n",
    "y_s_train = np.array([[1, 0]]*50 + [[0, 1]]*50)\n",
    "    \n",
    "enn = ENN(config)    \n",
    "W_trace, b_trace = enn.fit(x_s_train, y_s_train, params_trace_recording=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_fetch = (lambda: (yield 2))()\n",
    "\n",
    "_ = lambda: (yield 2)\n",
    "value_to_fetch = _()\n",
    "\n",
    "value_to_fetch = lambda: 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
