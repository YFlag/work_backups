{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Archive Version: 1.0\n",
    "This archive version is for seeing how ugly my code is before reconstruction...\n",
    "(⊙o⊙)…\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.output_wrapper {\n",
       " width:115%;\n",
       "}\n",
       "\n",
       "div.prompt,\n",
       ".prompt {  \n",
       " min-width: 0ex !important; \n",
       " width: 7.5ex !important;  \n",
       "}    \n",
       "\n",
       "div.out_prompt_overlay.prompt {\n",
       " min-width: 0ex !important;\n",
       " width: 4.5ex !important;\n",
       "}\n",
       "\n",
       "div.out_prompt_overlay.prompt:hover {\n",
       " min-width: 5.5ex !important;\n",
       " width: 5.5ex !important;\n",
       "}\n",
       "\n",
       "div.ui-dialog-titlebar {\n",
       " width:100%;\n",
       "}\n",
       "\n",
       "div.p-Widget.jupyter-widgets-output-area.output_wrapper {\n",
       " width:100%;\n",
       "}\n",
       "\n",
       "div.output_area pre{\n",
       " padding-left:5px;\n",
       "}\n",
       "\n",
       "p {\n",
       " margin-left:0.5em;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "from notebook_css import css\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2,
     5,
     18,
     21,
     26
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Design of the trail:\n",
    "1.Structure\n",
    "     - dimension mapping in the whole space transform process: (#1, #2, [...]). each # corresponds to an independent space, whose dimension is the value for the #.\n",
    "     - design of non-linear scalar-valued function design of each dimension above: polynomial functions.\n",
    "2.Fitting Algorithm\n",
    "     - gradient descent\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "We start by designing the polynomial function of degree 3. To consider all possible terms in polynomials, we can leverage the method of Cartesian product and make some changes on it.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "1. generate final result directly by something like `np.identity`?\n",
    "2. ideas about store type: \n",
    "    - pure ndarray implementation (order is clear).\n",
    "    - set(term_collection) -> set -> list.\n",
    "3. structure design of `term`:\n",
    "    - value('x_1'), power | matching by index -> not clear even possible.\n",
    "    - 'x_1', power | matching by str -> more clear, and matching by str is easy.\n",
    "    - [power1, power2, ...] for each idx -> less clear meaning, more memory, but more clear in structure. \n",
    "\"\"\"\n",
    "def get_terms_collection(quantity_of_variables, max_degree):\n",
    "    if max_degree == 1:\n",
    "        return [np.identity(quantity_of_variables, dtype=np.int32)]\n",
    "\n",
    "    \"\"\"\n",
    "    1. another idea for implementing `terms_collection_s`: add a new arg \\\n",
    "    'all_pre_terms_collection' in function.\n",
    "    2. `tcfcdl`: terms_collection_for_certain_degree_list\n",
    "    \"\"\"\n",
    "    tcfcdl = get_terms_collection(quantity_of_variables, max_degree - 1)\n",
    "\n",
    "    terms_collection_for_one_smaller_degree = tcfcdl[-1]\n",
    "    terms_collection_for_cur_max_degree = []\n",
    "\n",
    "    for idx in range(quantity_of_variables):\n",
    "        for pre_term in terms_collection_for_one_smaller_degree:\n",
    "            pre_term_ = pre_term.copy()\n",
    "            pre_term_[idx] += 1\n",
    "            terms_collection_for_cur_max_degree.append(pre_term_)\n",
    "        \"\"\"\n",
    "        another idea: generate `term_collector` by filtering something like \\\n",
    "        [1, 2], [2, 1] by set? seems wrong.\n",
    "        \"\"\"\n",
    "        terms_collection_for_one_smaller_degree = list(filter(\n",
    "            lambda term: term[idx] == 0,\n",
    "            terms_collection_for_one_smaller_degree\n",
    "        ))\n",
    "    tcfcdl.append(np.array(terms_collection_for_cur_max_degree))\n",
    "    return tcfcdl\n",
    "\n",
    "a = get_terms_collection(2, 2)\n",
    "\"\"\" 2000, 2 -> 2003000 \"\"\"\n",
    "a = np.vstack(a)\n",
    "# print(a)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ENN():\n",
    "    def __init__(self, **config):\n",
    "        \"\"\" initialize the default config. ref: see `simple_nn.ipynb` \"\"\"\n",
    "        self.config = lambda: 0\n",
    "\n",
    "        self.config.feature_dimension = 2\n",
    "        self.config.num_of_class = 2\n",
    "        self.config.space_mapping_process = (self.config.feature_dimension,\n",
    "                                             self.config.num_of_class)\n",
    "        self.config.max_degree = 1\n",
    "        self.config.learning_rate = 1\n",
    "        self.config.batch_size = 100\n",
    "        \n",
    "        \"\"\" override config with custom hyper-parameters if any. \"\"\"\n",
    "        for k, v in config.items():\n",
    "            if getattr(self.config, k, None):\n",
    "                setattr(self.config, k, v)\n",
    "            else:\n",
    "                raise AttributeError('Unknown hyper-parameter %s' % k)\n",
    "        \n",
    "        \"\"\" `fp_trasformation`: feature_polynomial_transformation \"\"\"\n",
    "        self.fp_transformation  = np.vstack(get_terms_collection(\n",
    "            self.config.feature_dimension, self.config.max_degree))\n",
    "        \n",
    "        \"\"\"\n",
    "        `W`: coefficients_for_all_dimension_in_next_space\n",
    "        len of `fp_transformation`: quantity_of_variable_term\n",
    "        \"\"\"\n",
    "        self.W = np.random.normal(size=(\n",
    "            len(self.fp_transformation), self.config.space_mapping_process[1]))\n",
    "        \n",
    "        \"\"\" `b`: constant_coefficient_for_all_dimension_in_next_space \"\"\"\n",
    "        self.b = np.random.random((self.config.space_mapping_process[1], ))\n",
    "        \n",
    "    def feature_transform(self, feature_input_s):\n",
    "        feature_output_s = []\n",
    "        for x in feature_input_s:\n",
    "            feature_output_s.append([\n",
    "                sum(var ** power if power != 0 else 0 for var, power in zip(\n",
    "                    x, term)) for term in self.fp_transformation\n",
    "            ])            \n",
    "        return np.array(feature_output_s)\n",
    "            \n",
    "    \"\"\" backpropagation \"\"\"\n",
    "    \"\"\" note that `x_s_train` is just an alias of `feature_input_s`. \"\"\"\n",
    "    def fit(self, x_s_train, y_s_train, *, steps=1000, \\\n",
    "            params_trace_recording=False):\n",
    "        x_s_train, y_s_train = np.array(x_s_train), np.array(y_s_train)\n",
    "        W_trace, b_trace = [self.W.copy()], [self.b.copy()]\n",
    "        W_gradients_trace, b_gradients_trace = [], []\n",
    "        sample_size = len(x_s_train)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            indices_batch = random.sample(\n",
    "                range(sample_size), self.config.batch_size)\n",
    "\n",
    "            \"\"\" i corresponds to row in `self.W` and j to col. \"\"\"\n",
    "            \"\"\" define them as function without actual object reference can \\\n",
    "            free memory at soon. and I think it's faster than del statement. \"\"\"\n",
    "            x_s = self.feature_transform(x_s_train[indices_batch])\n",
    "            y_minus_f_s = x_s.dot(self.W) + self.b - y_s_train[indices_batch]\n",
    "\n",
    "            \"\"\" \n",
    "            1. `pd`: partial derivative \n",
    "            2. `l_on_f`: all the loss_j on the f_j\n",
    "            3. `fj_on_w__j`: all the fj on the w_ij when j is fixed\n",
    "            \"\"\"\n",
    "            pd_of_l_on_f_s = \\\n",
    "            2 / self.config.space_mapping_process[1] * y_minus_f_s\n",
    "            pd_of_fj_on_w__j_s = x_s\n",
    "            \n",
    "            W_gradients = np.mean(\n",
    "                pd_of_fj_on_w__j_s[:, :, np.newaxis] @ \\\n",
    "                pd_of_l_on_f_s[:, np.newaxis, :], axis=0)\n",
    "            b_gradients = np.mean(\n",
    "                pd_of_l_on_f_s, axis=0)\n",
    "                        \n",
    "            self.W -= self.config.learning_rate * W_gradients\n",
    "            self.b -= self.config.learning_rate * b_gradients\n",
    "            \n",
    "            if params_trace_recording:\n",
    "                \"\"\" attention this! \"\"\"\n",
    "                W_trace.append(self.W.copy())\n",
    "                b_trace.append(self.b.copy())\n",
    "                W_gradients_trace.append(W_gradients.copy())\n",
    "                b_gradients_trace.append(b_gradients.copy())\n",
    "        print('training completed.') \n",
    "        return W_trace, b_trace, W_gradients_trace, b_gradients_trace\n",
    "\n",
    "    # forward computing        \n",
    "    def logits(self, x_s, W=None, b=None):\n",
    "        if W is not None and b is not None:\n",
    "            return self.feature_transform(x_s).dot(W) + \\\n",
    "                b\n",
    "        else:\n",
    "            return self.feature_transform(x_s).dot(self.W) + self.b\n",
    "\n",
    "    def predict(self, x_s, W=None, b=None):\n",
    "        return np.argmax(self.logits(x_s, W=W, b=b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     36,
     193,
     204
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-b1ea9a11315f>, line 86)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-b1ea9a11315f>\"\u001b[0;36m, line \u001b[0;32m86\u001b[0m\n\u001b[0;31m    for _ in range(len(W_trace):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "important notice: the matplotlib version is 2.1.0, and python version is 3.5.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import notebook_css\n",
    "import feature_visualization as fv\n",
    "from feature_visualization import md_display\n",
    "\n",
    "# fv.reset()\n",
    "# md_display('Step 1: Prepare the data set')\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5,5), sharex=True, sharey=True)\n",
    "# fv.style_initialize(ax)\n",
    "[x_coords, y_coords] = fv.sampling('quadratic', 'matplotlib', 50)\n",
    "xy1 = np.array([x_coords, y_coords+0.4])\n",
    "xy2 = np.array([x_coords, y_coords-0.45])\n",
    "# ax.plot(*xy1, 'bo', markersize=2, )\n",
    "# ax.plot(*xy2, 'ro', markersize=2, )\n",
    "# ax.legend(['class 1', 'class 2'])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "md_display('Step 2: Training Process')\n",
    "\"\"\" `x` here means input feature vector x \"\"\"\n",
    "x_s_train = np.hstack([xy1, xy2]).T\n",
    "y_s_train = np.array([[1, 0]]*50 + [[0, 1]]*50)\n",
    "enn = ENN(max_degree=1)    \n",
    "W_trace, b_trace, W_gradients_trace, b_gradients_trace = enn.fit(\n",
    "    x_s_train, y_s_train, steps=20, params_trace_recording=True)\n",
    "W_trace, b_trace = np.array(W_trace), np.array(b_trace)\n",
    "W_gradients_trace, b_gradients_trace = \\\n",
    "    np.array(W_gradients_trace), np.array(b_gradients_trace)\n",
    "\n",
    "\n",
    "md_display('Step 3: Training Process Plotting')\n",
    "def coefficients_format_conversion(W_trace, b_trace, origin):\n",
    "    \"\"\"\n",
    "    convert `W` and `b` (or their gradients) trace to matplotlib format.\n",
    "    :param origin: \n",
    "        0 or 1. the x coordinate of origin when plotting. 0 corresponds to \\\n",
    "        plotting gradients, and 1 to plotting weights.\n",
    "    \"\"\"\n",
    "    steps = range(origin, len(W_trace) + origin)\n",
    "    coeffs_all_dim = [] \n",
    "    for j_dim in range(W_trace.shape[2]):\n",
    "        coeffs_j_dim = []\n",
    "        for i_term in range(W_trace.shape[1]):\n",
    "            coeffs_j_dim += [steps, W_trace[:, i_term, j_dim]]\n",
    "        coeffs_j_dim += [steps, b_trace[:, j_dim]]\n",
    "        coeffs_all_dim.append(coeffs_j_dim.copy())\n",
    "    return np.array(coeffs_all_dim)\n",
    "\n",
    "def get_terms_for_display(terms):\n",
    "    legend = []\n",
    "    for i, term in enumerate(terms):\n",
    "        legend.append(r'$w(' + ''.join([\n",
    "            r'' if not pow_ \\\n",
    "                else r'x_{%(idx)s}' % {'idx': idx} if pow_ == 1 \\\n",
    "                else r'x^{%(pow)s}_{%(idx)s}' % {'pow': pow_, 'idx': idx} \\\n",
    "                for idx, pow_ in enumerate(term)\n",
    "        ]) + ')$')\n",
    "    legend.append(r'$w(bias_j)$')\n",
    "    return legend\n",
    "\n",
    "def params_plot(step):\n",
    "    print('gradients:')\n",
    "    steps_grad = range(1, len(W_gradients_trace))\n",
    "    term_num, dim_num = W_gradients_trace[0].shape\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11,5.5), sharex=True, sharey=True)\n",
    "    for dim_j in range(dim_num):\n",
    "        for term_i in range(term_num):\n",
    "            axes[dim_j].plot(\n",
    "                steps_grad, W_gradients_trace[:, term_i, dim_j],\n",
    "                label=r'$\\partial{w_{%s%s}}$' % (term_i, dim_j))\n",
    "        axes[dim_j].plot(\n",
    "            steps_grad, b_gradients_trace[:,dim_j], \n",
    "            label=r'$\\partial{bias_%s}$' % dim_j)\n",
    "    \n",
    "    print('gradients:')\n",
    "    steps_grad = range(1, len(W_gradients_trace))\n",
    "    term_num, dim_num = W_gradients_trace[0].shape\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11,5.5), sharex=True, sharey=True)\n",
    "    for dim_j in range(dim_num):\n",
    "        for term_i in range(term_num):\n",
    "            axes[dim_j].plot(\n",
    "                steps_grad, W_gradients_trace[:, term_i, dim_j],\n",
    "                label=r'$\\partial{w_{%s%s}}$' % (term_i, dim_j))\n",
    "        axes[dim_j].plot(\n",
    "            steps_grad, b_gradients_trace[:,dim_j], \n",
    "            label=r'$\\partial{bias_%s}$' % dim_j)\n",
    "    \n",
    "    \n",
    "    \n",
    "    axes[0].legend(\n",
    "        [r'$\\partial{w_{%s0}}$' %  i\n",
    "            for i in range(W_trace.shape[1])] \\\n",
    "               + [r'$\\partial{bias_0}$']\n",
    "    )\n",
    "    axes[0].set_title(r'dimension 0 of new space')\n",
    "    axes[0].set_ylabel('gradients')\n",
    "    axes[0].set_xlabel('step')\n",
    "\n",
    "    print('weights (generated by the trace of gradients):')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11,5.5), sharex=True, sharey=True)\n",
    "    for _ in range(len(W_trace):\n",
    "        axes[1].plot(*coefficients_format_conversion(W_trace, b_trace, 0))\n",
    "    axes[1].legend(get_terms_for_display(enn.fp_transformation))\n",
    "    axes[1].set_title(r'dimension 1 of new space')\n",
    "    axes[1].set_ylabel('weights')\n",
    "    axes[1].set_xlabel('step')\n",
    "\n",
    "# fv.jtplot.style('chesterish')\n",
    "params_plot(-1)\n",
    "# fv.reset()\n",
    "sampling_magnitude = 10\n",
    "grid = (sampling_magnitude, sampling_magnitude)\n",
    "xy_coords_test = fv.sampling('square', 'matplotlib', sampling_magnitude)\n",
    "handles=[\n",
    "    Patch(facecolor='b', label='class 1'),\n",
    "    Patch(facecolor='r', label='class 2'),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "labels_test = enn.predict(\n",
    "    xy_coords_test.T, W_trace[0], b_trace[0]).reshape(grid)\n",
    "f = lambda _: enn.logits(_.T, W_trace[0], b_trace[0]).T\n",
    "f1, f2, f3 = f(xy1), f(xy2), f(xy_coords_test)\n",
    "\n",
    "fv.reset()\n",
    "\"\"\" nonlinear scalar-valued function learned plotting \"\"\"\n",
    "fig = plt.figure(figsize=(11,5.5))\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "surf1 = ax1.plot_surface(*xy_coords_test.reshape(2, *grid), f3[0].reshape(grid),\n",
    "                cmap='coolwarm')\n",
    "fig.colorbar(surf1, shrink=0.5,)\n",
    "ax1.set_title('nonlinear function learned of dim 0', y=1.05)\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "surf2 = ax2.plot_surface(*xy_coords_test.reshape(2, *grid), f3[1].reshape(grid),\n",
    "                cmap='coolwarm')\n",
    "fig.colorbar(surf2, shrink=0.5,)\n",
    "ax2.set_title('nonlinear function learned of dim 1', y=1.05)\n",
    "fig.subplots_adjust(left=0.03, right=0.97, wspace=0.08)\n",
    "\n",
    "ax1._shared_z_axes.join(ax1, ax2)\n",
    "_ = (*ax1.get_zlim(), *ax2.get_zlim())\n",
    "new_zlim = min(_), max(_)\n",
    "ax1.set_zlim(new_zlim)\n",
    "\n",
    "fv.reset()\n",
    "\"\"\" decision boundary plotting (`db`: decision boundary) \"\"\"\n",
    "# fig_db, ax_db = plt.subplots(figsize=(5,5))\n",
    "# fv.style_initialize(ax_db)\n",
    "# db = ax_db.contourf(\n",
    "#     *xy_coords_test.reshape(2, *grid), labels_test, \n",
    "#     cmap='RdBu_r', alpha=0.6)\n",
    "# ax_db.plot(*xy1, 'bo', markersize=2, )\n",
    "# ax_db.plot(*xy2, 'ro', markersize=2, )\n",
    "# ax_db.legend(handles=handles)\n",
    "# acc_train = np.mean(\n",
    "#     enn.predict(x_s_train, W_trace[0], b_trace[0]) == np.argmax(y_s_train, 1))\n",
    "# print('1 Training Accuracy:', acc_train)\n",
    "\n",
    "\n",
    "\"\"\" transformation plotting \"\"\"\n",
    "\n",
    "fig_t, axes_t = plt.subplots(1, 2, figsize=(11,5.5), sharex=True, sharey=True)\n",
    "\"\"\" ref: https://stackoverflow.com/questions/18619880/matplotlib-adjust-figure-margin \"\"\"\n",
    "plt.subplots_adjust(left=0.03, right=0.97, wspace=0.08)\n",
    "# plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "fv.style_initialize(axes_t)\n",
    "\n",
    "lines_t =[]\n",
    "for ax, xy in zip(axes_t, [xy_coords_test, f3]):\n",
    "    t = ax.pcolormesh(*xy.reshape(2, *grid), labels_test, \n",
    "        cmap='coolwarm', edgecolor='white', alpha=0.5)\n",
    "    ax.grid(True, linestyle='--', alpha=1.)\n",
    "    ax.legend(handles=handles)\n",
    "axes_t[0].set_title('original space', y=1.05)\n",
    "axes_t[1].set_title('transformed space', y=1.05)\n",
    "axes_t[0].plot(*xy1, 'bo', markersize=1, )\n",
    "axes_t[0].plot(*xy2, 'ro', markersize=1, )\n",
    "lines_t.append(axes_t[1].plot(*f1, 'bo', markersize=2, ))\n",
    "lines_t.append(axes_t[1].plot(*f2, 'ro', markersize=2, ))\n",
    "#     print(f(np.array([x_coords, y_coords+0.4]))[:,:5])\n",
    "#     print(f(np.array([x_coords, y_coords-0.45]))[:,:5])\n",
    "con_s_1 = fv.connection_plot(axes_t[1], axes_t[0], f1, xy1)\n",
    "con_s_2 = fv.connection_plot(axes_t[1], axes_t[0], f2, xy2)\n",
    "\n",
    "\"\"\" `axis` will change data lim based current data. \"\"\"\n",
    "axes_t[0].axis('equal')\n",
    "axes_t[1].axis('equal')\n",
    "\n",
    "line = np.array([[-10,0,10], [-10,0,10]])\n",
    "pre_lim = axes_t[1].axis()\n",
    "for ax in axes_t: \n",
    "    \"\"\" plot will update the datalim. \"\"\"\n",
    "    ax.plot(*line, '--k', alpha=0.2, zorder=-1)\n",
    "    ax.plot([0], [0], 'ko', zorder=-1)\n",
    "\"\"\" priority: lim setter > sharex & sharey | autoscale_view\"\"\"\n",
    "axes_t[1].axis(pre_lim)\n",
    "\n",
    "# plt.show()\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "\n",
    "def decision_trace_plot(step):\n",
    "    \"\"\" `step`: the index of training step. \"\"\"\n",
    "    \"\"\" refreshing freq is too slow. how to optimize? \"\"\"\n",
    "    \n",
    "    labels_test = enn.predict(\n",
    "    xy_coords_test.T, W_trace[step], b_trace[step]).reshape(grid)\n",
    "    f = lambda _: enn.logits(_.T, W_trace[step], b_trace[step]).T\n",
    "    f1, f2, f3 = f(xy1), f(xy2), f(xy_coords_test)\n",
    "    \n",
    "#     fv.reset()\n",
    "#     global db\n",
    "# #     db.collections.clear()\n",
    "#     for _ in db.collections: _.remove()\n",
    "#     db = ax_db.contourf(\n",
    "#         *xy_coords_test.reshape(2, *grid), labels_test, \n",
    "#         cmap='RdBu_r', alpha=0.6)\n",
    "#     acc_train = np.mean(\n",
    "#     enn.predict(x_s_train, W_trace[step], b_trace[step]) == np.argmax(y_s_train, 1))\n",
    "#     print('Training Accuracy:', acc_train)\n",
    "    \n",
    "#     for ax in axes_t: set_size(8, 4, ax)\n",
    "    global t\n",
    "    t.remove()\n",
    "    t = axes_t[1].pcolormesh(*f3.reshape(2, *grid), labels_test, \n",
    "        cmap='coolwarm', edgecolor='white', alpha=0.5)\n",
    "    axes_t[1].grid(True, linestyle='--', alpha=1.)\n",
    "#         ax.lines.remove(l_diag)\n",
    "    \n",
    "    new_xlim = min(-1, f3[0].min()), max(1, f3[0].max())\n",
    "    new_ylim = min(-1, f3[1].min()), max(1, f3[1].max())\n",
    "#     new_xlim = np.array(new_xlim)\n",
    "#     new_ylim = np.array(new_ylim)\n",
    "    \"\"\" figsize, dpi, set_lim \"\"\"\n",
    "    \"\"\" css, ax size, tight \"\"\"\n",
    "    \n",
    "    \"\"\" set_[]lim, ax.axis, updata_datalim, update_from_..., margin \"\"\"\n",
    "    \n",
    "    \"\"\" not work. \"\"\"\n",
    "#     axes_t[0].update_datalim([new_xlim, new_ylim])\n",
    "#     axes_t[1].update_datalim([new_xlim, new_ylim])\n",
    "    \n",
    "    \"\"\" only `set_lim` and `update_...` together can work? \"\"\"\n",
    "#     axes_t[0].set_xlim(new_xlim)\n",
    "#     axes_t[0].set_ylim(new_ylim)\n",
    "#     axes_t[1].set_xlim(new_xlim)\n",
    "#     axes_t[1].set_ylim(new_ylim)\n",
    "    \n",
    "    \"\"\" seems the same as above. \"\"\"\n",
    "    for ax in axes_t:\n",
    "        ax.axis([*new_xlim, *new_ylim])\n",
    "    \n",
    "    \"\"\" ref: https://stackoverflow.com/questions/7386872/make-matplotlib-autoscaling-ignore-some-of-the-plots \"\"\"\n",
    "    axes_t[0].dataLim.update_from_data_xy(f3.T, ignore=True)\n",
    "    axes_t[1].dataLim.update_from_data_xy(f3.T, ignore=True)\n",
    "    \n",
    "    \"\"\" not work. \"\"\"\n",
    "#     for ax in axes_t: ax.margins(0.2)\n",
    "    \n",
    "    print('interval:', axes_t[1].xaxis.get_data_interval())\n",
    "    print('interval:', axes_t[1].yaxis.get_data_interval())\n",
    "    \n",
    "    \n",
    "#     xmin, xmax = min(-1, f3[0].min())-0.06, max(1, f3[0].max())+0.06\n",
    "#     ymin, ymax = min(-1, f3[1].min())-0.06, max(1, f3[1].max())+0.06\n",
    "#     axes_t[0].set_xlim(xmin=xmin, xmax=xmax)\n",
    "#     axes_t[0].set_ylim(ymin=ymin, ymax=ymax)\n",
    "#     axes_t[1].set_xlim(xmin=xmin, xmax=xmax)\n",
    "#     axes_t[1].set_ylim(ymin=ymin, ymax=ymax)\n",
    "    \n",
    "#     lines_diag.clear()\n",
    "#     for _ in lines_diag: _.set_visible(False)\n",
    "    print('0',axes_t[0].axis())\n",
    "    print('1',axes_t[1].axis())\n",
    "    \n",
    "#     print(ax.relim(visible_only=True))\n",
    "#     ax.autoscale()\n",
    "#     print('2', ax.axis())\n",
    "#     for _ in lines_diag: _.set_visible(True)\n",
    "    axes_t[1].lines[0].set_data(*f1)\n",
    "    axes_t[1].lines[1].set_data(*f2)\n",
    "    for con, xy in zip(con_s_1, f1.T):\n",
    "        con.xy1 = xy\n",
    "    for con, xy in zip(con_s_2, f2.T):\n",
    "        con.xy1 = xy\n",
    "        \n",
    "        \n",
    "#     fig_t.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "widgets.interact(\n",
    "    decision_trace_plot, step=widgets.IntSlider(min=0, max=len(W_trace)-1),)\n",
    "pass\n",
    "# print(W_trace[-3:])\n",
    "# print(b_trace[-3:])\n",
    "print(get_terms_for_display(enn.fp_transformation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Following Work:\n",
    "- visualize the gradients, weights, bias. (1)\n",
    "- multiple graphs visualization in the case of max degree of freedom > 2. (2)\n",
    "- visualize the space transformation. (3)\n",
    "- increase the degree of freedom of ENN.\n",
    "- try to analyze the 3-d data.\n",
    "- XOR problem.\n",
    "- add softmax, observe the independence among the gradients, and so on.\n",
    "- spiral curve problem, and so on.\n",
    "- automatic derivatives calculation using sympy.\n",
    "- optimize the calculation of value of the polynomial terms.\n",
    "- linear cls (two line) & linear regression?\n",
    "- would it converge faster when swapping the label blue and label red?\n",
    "- remove the bias.\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design of Visualization Experiment\n",
    "1.feature\n",
    "- histogram, space distribution, image-gram\n",
    "\n",
    "2.weights, bias\n",
    "- image-gram, histogram\n",
    "- trace, gradients trace\n",
    "\n",
    "3.transformed feature\n",
    "- historam, space transformation, image-gram\n",
    "- non-linear activation function learned dimension-wise in new space\n",
    "\n",
    "4.softmax probability\n",
    "- space transformation, classification boundary\n",
    "\n",
    "5.loss, loss-sample-wise\n",
    "- trace with step, trace with weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
